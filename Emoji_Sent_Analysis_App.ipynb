{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushJkumar/Emoji-Sentiment-Analysis/blob/main/Emoji_Sent_Analysis_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayslA9H0bAyy",
        "outputId": "ad884735-e7cc-413c-ff52-af1fc3c1f5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Emoji-Sentiment-Analysis'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 41 (delta 15), reused 19 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (41/41), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aayushJkumar/Emoji-Sentiment-Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aku3Bn40bE2D",
        "outputId": "654b4db2-b88a-446b-e16b-734eb9dca307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit==1.7.0\n",
            "  Downloading streamlit-1.7.0-py2.py3-none-any.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 9.0 MB/s \n",
            "\u001b[?25hCollecting streamlit_webrtc==0.22.1\n",
            "  Downloading streamlit_webrtc-0.22.1-py3-none-any.whl (867 kB)\n",
            "\u001b[K     |████████████████████████████████| 867 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 78.4 MB/s \n",
            "\u001b[?25hCollecting unicode\n",
            "  Downloading unicode-2.9-py2.py3-none-any.whl (14 kB)\n",
            "Collecting contractions==0.1.72\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting spacy==3.4.2\n",
            "  Downloading spacy-3.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 55.0 MB/s \n",
            "\u001b[?25hCollecting emot==3.1\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 23 kB/s \n",
            "\u001b[?25hCollecting emoji==2.1.0\n",
            "  Downloading emoji-2.1.0.tar.gz (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 77.0 MB/s \n",
            "\u001b[?25hCollecting expertai-nlapi\n",
            "  Downloading expertai_nlapi-2.4.1-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (3.17.3)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (4.13.0)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (5.1.1)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (21.3)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (6.0.1)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 75.2 MB/s \n",
            "\u001b[?25hCollecting blinker\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (4.2.4)\n",
            "Collecting validators\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (22.1.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (7.1.2)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (1.5.1)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0b4-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (7.1.2)\n",
            "Collecting aiortc<2.0.0,>=1.1.2\n",
            "  Downloading aiortc-1.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 57.1 MB/s \n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (1.0.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (1.9.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (8.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (4.64.1)\n",
            "Collecting cryptography>=2.2\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 47.5 MB/s \n",
            "\u001b[?25hCollecting pyee>=9.0.0\n",
            "  Downloading pyee-9.0.4-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from aiortc<2.0.0,>=1.1.2->streamlit_webrtc==0.22.1->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 2)) (1.15.1)\n",
            "Collecting pylibsrtp>=0.5.6\n",
            "  Downloading pylibsrtp-0.7.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting aioice<0.8.0,>=0.7.5\n",
            "  Downloading aioice-0.7.6-py3-none-any.whl (23 kB)\n",
            "Collecting google-crc32c>=1.1\n",
            "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Collecting av<10.0.0,>=9.0.0\n",
            "  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting netifaces\n",
            "  Downloading netifaces-0.11.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (32 kB)\n",
            "Collecting dnspython>=2.0.0\n",
            "  Downloading dnspython-2.2.1-py3-none-any.whl (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (3.9.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->aiortc<2.0.0,>=1.1.2->streamlit_webrtc==0.22.1->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 2)) (2.21)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (5.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (2022.4)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (5.2.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (2022.9.24)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 76.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.2->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 6)) (0.7.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit==1.7.0->-r /content/Emoji-Sentiment-Analysis/requirements.txt (line 1)) (4.4.2)\n",
            "Building wheels for collected packages: emoji, pyngrok, validators\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.1.0-py3-none-any.whl size=212392 sha256=2e1fc2da58223af699438b405dbe93ba84e22b8a3a7c9192e80c6e6bfa5b69e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/75/99/51c2a119f4cfd3af7b49cc57e4f737bed7e40b348a85d82804\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=f91282be646480e635a0d6d47b3380f60fcccf6c73d219e4de0112df91cfb4f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=cdbf0c1a146a04d2b3ef072e3bedb24d4b61d264ee677ee687adaf9acaf4520e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
            "Successfully built emoji pyngrok validators\n",
            "Installing collected packages: typing-extensions, smmap, netifaces, gitdb, dnspython, watchdog, validators, semver, pympler, pylibsrtp, pyee, pydeck, pyahocorasick, google-crc32c, gitpython, cryptography, blinker, base58, av, anyascii, aioice, textsearch, streamlit, aiortc, unicode, streamlit-webrtc, spacy, pyngrok, expertai-nlapi, emot, emoji, contractions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "Successfully installed aioice-0.7.6 aiortc-1.3.2 anyascii-0.3.1 av-9.2.0 base58-2.1.1 blinker-1.5 contractions-0.1.72 cryptography-38.0.1 dnspython-2.2.1 emoji-2.1.0 emot-3.1 expertai-nlapi-2.4.1 gitdb-4.0.9 gitpython-3.1.29 google-crc32c-1.5.0 netifaces-0.11.0 pyahocorasick-1.4.4 pydeck-0.8.0b4 pyee-9.0.4 pylibsrtp-0.7.1 pympler-1.0.1 pyngrok-5.1.0 semver-2.13.0 smmap-5.0.0 spacy-3.4.2 streamlit-1.7.0 streamlit-webrtc-0.22.1 textsearch-0.0.24 typing-extensions-3.10.0.2 unicode-2.9 validators-0.20.0 watchdog-2.1.9\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r /content/Emoji-Sentiment-Analysis/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q3nWB1SbK9J"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import cPickle as pickle\n",
        "except ImportError:\n",
        "    import pickle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_lubnuZbbWp",
        "outputId": "1cbc2193-7a52-45c2-e7ba-753d944afcf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEa6Xj7Hbf3v",
        "outputId": "c5abba29-1d75-4c69-8211-a8b9992ec91b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2FnIZS2sOzC23RY3tQsbIT2J1yl_85dr72YeTbA9xbbVuWKC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UCxwkoicEew",
        "outputId": "66d1af40-e504-43a3-9c2c-7ef40980de06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NgrokTunnel: \"https://f68f-34-83-119-78.ngrok.io\" -> \"http://localhost:80\"\n",
            "2022-10-25 03:01:41.587 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "2022-10-25 03:04:40.993872: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-10-25 03:09:49.690 Response status code: 403\n",
            "2022-10-25 03:09:49.695 Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/script_runner.py\", line 430, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Emoji-Sentiment-Analysis/app.py\", line 36, in <module>\n",
            "    main()\n",
            "  File \"/content/Emoji-Sentiment-Analysis/app.py\", line 16, in main\n",
            "    df=file_input(file)\n",
            "  File \"/content/Emoji-Sentiment-Analysis/functions.py\", line 209, in file_input\n",
            "    df[\"score\"] = df['clean_text'].apply(lambda x: calculate_sentiment(x))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\", line 4357, in apply\n",
            "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\", line 1043, in apply\n",
            "    return self.apply_standard()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\", line 1101, in apply_standard\n",
            "    convert=self.convert_dtype,\n",
            "  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n",
            "  File \"/content/Emoji-Sentiment-Analysis/functions.py\", line 209, in <lambda>\n",
            "    df[\"score\"] = df['clean_text'].apply(lambda x: calculate_sentiment(x))\n",
            "  File \"/content/Emoji-Sentiment-Analysis/functions.py\", line 40, in calculate_sentiment\n",
            "    params={'language': language, 'resource': 'sentiment'})\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/expertai/nlapi/cloud/client.py\", line 118, in specific_resource_analysis\n",
            "    return self.process_response(response)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/expertai/nlapi/cloud/client.py\", line 81, in process_response\n",
            "    \"Response status code: {}\".format(response.status_code)\n",
            "expertai.nlapi.common.errors.ExpertAiRequestError: Response status code: 403\n",
            "\n",
            "2022-10-25 03:10:25.206 Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\", line 385, in get_loc\n",
            "    return self._range.index(new_key)\n",
            "ValueError: 20 is not in range\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/script_runner.py\", line 430, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Emoji-Sentiment-Analysis/app.py\", line 36, in <module>\n",
            "    main()\n",
            "  File \"/content/Emoji-Sentiment-Analysis/app.py\", line 30, in main\n",
            "    score=\"Sentiment Score : \"+str(df2.loc[number1-1,'score'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 925, in __getitem__\n",
            "    return self._getitem_tuple(key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 1100, in _getitem_tuple\n",
            "    return self._getitem_lowerdim(tup)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 838, in _getitem_lowerdim\n",
            "    section = self._getitem_axis(key, axis=i)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 1164, in _getitem_axis\n",
            "    return self._get_label(key, axis=axis)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 1113, in _get_label\n",
            "    return self.obj.xs(label, axis=axis)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\", line 3776, in xs\n",
            "    loc = index.get_loc(key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\", line 387, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 20\n",
            "\n",
            "2022-10-25 03:10:53.233 Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\", line 385, in get_loc\n",
            "    return self._range.index(new_key)\n",
            "ValueError: 1 is not in range\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/script_runner.py\", line 430, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Emoji-Sentiment-Analysis/app.py\", line 36, in <module>\n",
            "    main()\n",
            "  File \"/content/Emoji-Sentiment-Analysis/app.py\", line 30, in main\n",
            "    score=\"Sentiment Score : \"+str(df2.loc[number1-1,'score'])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 925, in __getitem__\n",
            "    return self._getitem_tuple(key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 1100, in _getitem_tuple\n",
            "    return self._getitem_lowerdim(tup)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 838, in _getitem_lowerdim\n",
            "    section = self._getitem_axis(key, axis=i)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 1164, in _getitem_axis\n",
            "    return self._get_label(key, axis=axis)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\", line 1113, in _get_label\n",
            "    return self.obj.xs(label, axis=axis)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\", line 3776, in xs\n",
            "    loc = index.get_loc(key)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\", line 387, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 1\n",
            "\n",
            "2022-10-25 03:13:15.514 Response status code: 403\n",
            "2022-10-25 03:13:15.514 Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/streamlit/script_runner.py\", line 430, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/Emoji-Sentiment-Analysis/app.py\", line 36, in <module>\n",
            "    main()\n",
            "  File \"/content/Emoji-Sentiment-Analysis/app.py\", line 16, in main\n",
            "    df=file_input(file)\n",
            "  File \"/content/Emoji-Sentiment-Analysis/functions.py\", line 209, in file_input\n",
            "    df[\"score\"] = df['clean_text'].apply(lambda x: calculate_sentiment(x))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\", line 4357, in apply\n",
            "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\", line 1043, in apply\n",
            "    return self.apply_standard()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\", line 1101, in apply_standard\n",
            "    convert=self.convert_dtype,\n",
            "  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n",
            "  File \"/content/Emoji-Sentiment-Analysis/functions.py\", line 209, in <lambda>\n",
            "    df[\"score\"] = df['clean_text'].apply(lambda x: calculate_sentiment(x))\n",
            "  File \"/content/Emoji-Sentiment-Analysis/functions.py\", line 40, in calculate_sentiment\n",
            "    params={'language': language, 'resource': 'sentiment'})\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/expertai/nlapi/cloud/client.py\", line 118, in specific_resource_analysis\n",
            "    return self.process_response(response)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/expertai/nlapi/cloud/client.py\", line 81, in process_response\n",
            "    \"Response status code: {}\".format(response.status_code)\n",
            "expertai.nlapi.common.errors.ExpertAiRequestError: Response status code: 403\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port='80',bind_tls=True)\n",
        "print (public_url)\n",
        "!cd Emoji-Sentiment-Analysis && streamlit run --server.port 80 app.py >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOBC309XcNFB"
      },
      "outputs": [],
      "source": [
        "!pgrep streamlit\n",
        "\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfnHzvEwdYTJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyN5haLBpu5afKhc3wTEo59v",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}